{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd778c70-25fd-4e05-b8a7-db961f58d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a1ffaf5-2a58-4df5-9b55-afdbcd027e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('gold_data_annotated_hyperbole.tsv', sep='\\t')\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['tagged_comment'].tolist(),\n",
    "    df['hyperbole_label'].tolist(),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68fd0cb-e0ba-4005-8081-6ccbc7e62ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a custom dataset\n",
    "class HyperboleDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = HyperboleDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = HyperboleDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61e3b6b8-7ac9-4439-9acd-cad6911b0e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Dataset Distribution:\n",
      "Label 1: 363 (24.22%)\n",
      "Label 0: 1136 (75.78%)\n",
      "\n",
      "Training Dataset Distribution:\n",
      "Label 0: 898 (74.90%)\n",
      "Label 1: 301 (25.10%)\n",
      "\n",
      "Validation Dataset Distribution:\n",
      "Label 0: 238 (79.33%)\n",
      "Label 1: 62 (20.67%)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Function to calculate and print label distribution\n",
    "def print_label_distribution(labels, dataset_name):\n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    print(f\"\\n{dataset_name} Dataset Distribution:\")\n",
    "    for label, count in counter.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"Label {label}: {count} ({percentage:.2f}%)\")\n",
    "\n",
    "# Print distributions\n",
    "print_label_distribution(df['hyperbole_label'].tolist(), \"Full\")\n",
    "print_label_distribution(train_labels, \"Training\")\n",
    "print_label_distribution(val_labels, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9958f4ba-9492-40c4-9363-384aa0852985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/python3-8-19/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 completed. Average Loss: 0.5392\n",
      "End of Epoch 1/3\n",
      "Validation Accuracy: 0.7967\n",
      "Validation Macro F1: 0.4591\n",
      "Validation Weighted F1: 0.7098\n",
      "Epoch 2/3 completed. Average Loss: 0.4236\n",
      "End of Epoch 2/3\n",
      "Validation Accuracy: 0.8167\n",
      "Validation Macro F1: 0.6231\n",
      "Validation Weighted F1: 0.7815\n",
      "Epoch 3/3 completed. Average Loss: 0.2331\n",
      "End of Epoch 3/3\n",
      "Validation Accuracy: 0.7900\n",
      "Validation Macro F1: 0.6817\n",
      "Validation Weighted F1: 0.7906\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def compute_metrics(true_labels, predictions):\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    weighted_f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "    return accuracy, macro_f1, weighted_f1\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "print_every = 100  # Print stats every 100 batches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader, 1):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % print_every == 0:\n",
    "            avg_loss = total_loss / batch_idx\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                  f\"Avg Loss: {avg_loss:.4f}, Time: {elapsed:.2f}s\")\n",
    "\n",
    "            # Quick validation check\n",
    "            model.eval()\n",
    "            val_predictions = []\n",
    "            val_true_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_loader:\n",
    "                    val_input_ids = val_batch['input_ids'].to(device)\n",
    "                    val_attention_mask = val_batch['attention_mask'].to(device)\n",
    "                    val_labels = val_batch['labels'].to(device)\n",
    "\n",
    "                    val_outputs = model(val_input_ids, attention_mask=val_attention_mask)\n",
    "                    _, val_preds = torch.max(val_outputs.logits, dim=1)\n",
    "\n",
    "                    val_predictions.extend(val_preds.cpu().tolist())\n",
    "                    val_true_labels.extend(val_labels.cpu().tolist())\n",
    "\n",
    "            val_accuracy, val_macro_f1, val_weighted_f1 = compute_metrics(val_true_labels, val_predictions)\n",
    "            print(f\"Quick Validation Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Quick Validation Macro F1: {val_macro_f1:.4f}\")\n",
    "            print(f\"Quick Validation Weighted F1: {val_weighted_f1:.4f}\")\n",
    "\n",
    "            model.train()  # Set the model back to training mode\n",
    "\n",
    "    # End of epoch\n",
    "    avg_epoch_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Full validation at the end of each epoch\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            val_predictions.extend(preds.cpu().tolist())\n",
    "            val_true_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    val_accuracy, val_macro_f1, val_weighted_f1 = compute_metrics(val_true_labels, val_predictions)\n",
    "    print(f\"End of Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation Macro F1: {val_macro_f1:.4f}\")\n",
    "    print(f\"Validation Weighted F1: {val_weighted_f1:.4f}\")\n",
    "\n",
    "# ... (rest of the code remains the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2d2cc95-bfa7-4845-b6be-e2aeff0ef541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to hyperbole_model_nfl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# ... (previous code remains the same)\n",
    "\n",
    "# After training is complete\n",
    "\n",
    "# Define the directory to save the model\n",
    "save_directory = 'hyperbole_model_nfl'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "# Save additional information\n",
    "model_info = {\n",
    "    'model_name': 'BERT for Hyperbole Detection',\n",
    "    'base_model': 'bert-base-uncased',\n",
    "    'num_labels': 2,\n",
    "    'training_params': {\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': 16,  # Adjust if you've changed this\n",
    "        'learning_rate': 2e-5,  # Adjust if you've changed this\n",
    "    },\n",
    "    'performance': {\n",
    "        'final_validation_accuracy': val_accuracy\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_directory, 'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_directory}\")\n",
    "\n",
    "# To load the model later, you can use:\n",
    "# loaded_model = BertForSequenceClassification.from_pretrained('hyperbole_model_nfl')\n",
    "# loaded_tokenizer = BertTokenizer.from_pretrained('hyperbole_model_nfl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd3be3-4330-4a64-a987-5b098370f4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3-8-19",
   "language": "python",
   "name": "python-3-8-19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
